{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "868e07b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "763bae16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['sms', 'label'],\n",
      "    num_rows: 4459\n",
      "}) Dataset({\n",
      "    features: ['sms', 'label'],\n",
      "    num_rows: 1115\n",
      "})\n",
      "Dataset({\n",
      "    features: ['sms', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 4459\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Code responsible for loading the dataset where seed means the random seed for splitting \n",
    "# the dataset to get the same split every time and test size is the size of the test set where 0.2 means 20% of the dataset is used for testing\n",
    "\n",
    "dataset= load_dataset(\"sms_spam\", split = \"train\").train_test_split(\n",
    "    test_size=0.2, shuffle=True, seed=42)\n",
    "\n",
    "print(dataset[\"train\"], dataset[\"test\"])\n",
    "\n",
    "\n",
    "# Code responsible for tokenizing the dataset using the tokenizer from the model\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Set the pad_token to eos_token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Define the splits\n",
    "splits = [\"train\", \"test\"]\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_dataset = {}\n",
    "for split in splits:\n",
    "    tokenized_dataset[split] = dataset[split].map(\n",
    "        lambda x: tokenizer(x[\"sms\"], truncation=True, padding=\"max_length\"),\n",
    "        batched=True\n",
    "    )\n",
    "\n",
    "print(tokenized_dataset[\"train\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5271fcb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2ForSequenceClassification(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2SdpaAttention(\n",
      "          (c_attn): Conv1D(nf=2304, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=768)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D(nf=3072, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=3072)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (score): Linear(in_features=768, out_features=2, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Code for loading the model and setting the model parameters\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"gpt2\", num_labels=2, id2label={0: \"not spam\", 1: \"spam\"}, label2id={\"not spam\": 0, \"spam\": 1})\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Unfreeze all the model parameters.\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b07666f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nav/.local/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_59103/216692262.py:9: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# Code for defining the compute_metrics function which will be used to compute \n",
    "# the accuracy of the model\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    return {\"accuracy\": (preds == labels).mean()}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\".data/spam_not_spam\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        num_train_epochs=1,\n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True,\n",
    "        logging_dir=\"logs\",\n",
    "        fp16=True,\n",
    "    ),\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abd5508f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2c32a5afd1f45fda60b892849ec8480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Select random samples\n",
    "random_indices = np.random.choice(len(dataset[\"test\"]), size=100, replace=False)\n",
    "items_for_manual_review = dataset[\"test\"].select(random_indices)\n",
    "\n",
    "# Create a dataset with the required format\n",
    "def prepare_data_for_prediction(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"sms\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "        return_tensors=None  # Important: return lists, not tensors\n",
    "    )\n",
    "    tokenized[\"labels\"] = examples[\"label\"]\n",
    "    return tokenized\n",
    "\n",
    "# Convert to proper dataset format\n",
    "review_dataset = Dataset.from_dict({\n",
    "    \"sms\": items_for_manual_review[\"sms\"],\n",
    "    \"label\": items_for_manual_review[\"label\"]\n",
    "})\n",
    "\n",
    "# Tokenize and format\n",
    "tokenized_review_dataset = review_dataset.map(\n",
    "    prepare_data_for_prediction,\n",
    "    batched=True,\n",
    "    remove_columns=review_dataset.column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5cf3670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d12217859770484ca05aa3fce7b80dae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eca17d7b9b604e15a83b2f13b2cec40f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/279 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results saved to: /home/nav/Projects_1/GenAI/PEFT_gp2/foundationalmodel_prediction_results.json\n"
     ]
    }
   ],
   "source": [
    "# Get predictions\n",
    "results = trainer.predict(tokenized_review_dataset)\n",
    "\n",
    "# Get evaluation results and prepare data for JSON\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "# Convert predictions to DataFrame\n",
    "predictions = np.argmax(results.predictions, axis=1)\n",
    "df = pd.DataFrame({\n",
    "    \"sms\": items_for_manual_review[\"sms\"],\n",
    "    \"label\": items_for_manual_review[\"label\"],\n",
    "    \"prediction\": predictions\n",
    "})\n",
    "\n",
    "# Create a dictionary with both evaluation and prediction results\n",
    "final_results = {\n",
    "    \"model_evaluation\": eval_results,\n",
    "    \"predictions\": df.to_dict(orient='records')\n",
    "}\n",
    "\n",
    "# Save to JSON with indentation for readability\n",
    "json_output_path = os.path.join(os.getcwd(), \"foundationalmodel_prediction_results.json\")\n",
    "with open(json_output_path, 'w') as f:\n",
    "    json.dump(final_results, f, indent=4)\n",
    "print(f\"\\nResults saved to: {json_output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
